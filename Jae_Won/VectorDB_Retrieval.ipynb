{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "17b9ed7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 첫 번째 셀: 라이브러리 임포트 및 환경 설정\n",
    "from typing import Annotated, List, TypedDict, Dict\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_pinecone import Pinecone as PineconeVectorStore\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import CrossEncoderReranker\n",
    "from langchain_community.cross_encoders import HuggingFaceCrossEncoder\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import asyncio\n",
    "import json\n",
    "import re\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168f3df3",
   "metadata": {},
   "source": [
    "## * 임베딩 모델 후보(현재는 적재 OpenAI Embedding으로해서, 쿼리도 반드시 같은 임베딩으로 해야함)\n",
    "- OpenAI Embedding(text-embedding-ada-002) \n",
    "- PubMedBERT Embeddings -> PubMed 원문으로 사전학습·문장단위 768-d 임베딩\n",
    "- BioSimCSE-BioLinkBERT -> BioLinkBERT에 SimCSE 대조학습 적용\n",
    "- BioBERT-NLI -> BioBERT 기반 + NLI 세트로 파인튜닝\n",
    "- Multilingual E5-base -> E5 구조·12 L·768 d·다국어 지원\n",
    "- BMRetriever -> LLM 기반 Bio-전용 dense retriever\n",
    "\n",
    "## * 리랭커 모델 후보\n",
    "- MedCPT Cross-Encoder -> PubMedBERT 초기화·18 M 쿼리-문서로 학습\n",
    "- PubMedBERT Cross-Enc -> PubMed 검색 로그 하드네거티브로 파인튜닝\n",
    "- BGE reranker-v2-gemma -> Gemma-2 B 백본·다국어 최적화\n",
    "- monoT5-large -> Seq2Seq pointwise 랭커\n",
    "- mxbai-rerank-base-v1 -> DeBERTa 125 M 경량 모델\n",
    "- Rank/ListT5 (11 B) -> Listwise·Fusion-in-Decoder\n",
    "\n",
    "## * Pinecone 안에서 리트리버 변형하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "661ff3b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retriever 생성 중\n",
      "Pinecone 연결 완료\n",
      "Retriever 생성 완료\n"
     ]
    }
   ],
   "source": [
    "# 두 번째 셀: 핵심 Retriever 생성\n",
    "def create_retriever():\n",
    "    print(\"Retriever 생성 중\")\n",
    "    \n",
    "    # 1536차원 임베딩 사용 (인덱스와 일치)\n",
    "    embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "    \n",
    "    # 올바른 설정\n",
    "    pinecone_vs = PineconeVectorStore.from_existing_index(\n",
    "        index_name=\"boazpubmed\",\n",
    "        embedding=embeddings,\n",
    "        namespace=\"\",  # 빈 네임스페이스 (26483개 문서가 있는 곳)\n",
    "        text_key=\"page_content\"  # 실제 텍스트가 저장된 키\n",
    "    )\n",
    "    \n",
    "    print(\"Pinecone 연결 완료\")\n",
    "\n",
    "    # Reranker 설정(Pubmed 기반으로 수정)\n",
    "    reranker = HuggingFaceCrossEncoder(model_name=\"ncbi/MedCPT-Cross-Encoder\") #MedCPT-Cross-Encoder 리랭커 사용용\n",
    "    compressor = CrossEncoderReranker(model=reranker, top_n=4)\n",
    "\n",
    "    base = pinecone_vs.as_retriever(search_kwargs={\"k\": 10})\n",
    "    compression_retriever = ContextualCompressionRetriever(\n",
    "        base_retriever=base,\n",
    "        base_compressor=compressor\n",
    "    )\n",
    "    \n",
    "    print(\"Retriever 생성 완료\")\n",
    "    return compression_retriever\n",
    "\n",
    "# 전역 retriever 생성\n",
    "retriever = create_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "409ad772",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 세 번째 셀 ▸ NICHD xls → 사전 & 쿼리 재작성기\n",
    "import pandas as pd, re, json, sys, subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "# 1) 동의어 사전 구축(NIH 제공해주는 Pediatric Terminology.xls 파일 사용) + (UMLS API로 동의어 사전 확장 가능, 아직 미적용)\n",
    "XLS_PATH = (r\"C:\\Users\\user\\Desktop\\BOAZ\\23기 분석 ADV\\소아마취 챗봇 프로젝트\"\n",
    "            r\"\\소아마취 Agent\\Pediatric_Terminology.xls\")\n",
    "\n",
    "def build_term_dict_from_xls(xls_path: str):\n",
    "    term_dict: Dict[str, List[str]] = {}\n",
    "    sheets = pd.read_excel(xls_path, sheet_name=None, engine=\"xlrd\")  # 엔진 고정\n",
    "    for df in sheets.values():\n",
    "        df = df.rename(columns=lambda c: c.strip())\n",
    "        if {\"Peds Preferred Term\", \"Peds Synonym\"} <= set(df.columns):\n",
    "            for pref, syns in zip(df[\"Peds Preferred Term\"], df[\"Peds Synonym\"]):\n",
    "                if pd.isna(pref):\n",
    "                    continue\n",
    "                pref = str(pref).strip().lower()\n",
    "                syn_list: List[str] = []\n",
    "                if isinstance(syns, str):\n",
    "                    syn_list = re.split(r\"[|;,]\", syns)\n",
    "                elif not pd.isna(syns):\n",
    "                    syn_list = [str(syns)]\n",
    "                syn_list = [s.strip().lower() for s in syn_list if s.strip()]\n",
    "                term_dict.setdefault(pref, []).extend(syn_list + [pref])\n",
    "    return {k: sorted(set(v)) for k, v in term_dict.items()}\n",
    "\n",
    "PEDIATRIC_TERMS = build_term_dict_from_xls(XLS_PATH)\n",
    "\n",
    "# 2) 쿼리 한글 → 영어 선번역 유틸\n",
    "async def ko2en(text: str, model_name: str = \"gpt-4o-mini\"):\n",
    "    \"\"\"\n",
    "    한글 의료 질문을 영어로 자연스러운 전문 용어 중심 표현으로 번역\n",
    "    \"\"\"\n",
    "    llm = ChatOpenAI(model_name=model_name, temperature=0.0, max_tokens=300)\n",
    "    resp = await llm.ainvoke(f\"Translate the following pediatric-anesthesia question to English \"\n",
    "                             f\"(keep medical terms and abbreviations):\\n{text}\")\n",
    "    return resp.content.strip()\n",
    "\n",
    "# 3) 메인 재작성 클래스\n",
    "class GPTQueryRewriter:\n",
    "    \"\"\"한국어,영어 입력 모두 처리 → 영어 OR 확장 벡터 검색 쿼리 반환\"\"\"\n",
    "    def __init__(self, model_name: str = \"gpt-4o-mini\", temperature: float = 0.2):\n",
    "        self.llm = ChatOpenAI(model_name=model_name,\n",
    "                              temperature=temperature,\n",
    "                              max_tokens=300)\n",
    "\n",
    "    async def rewrite(self, question, mode = \"initial\"):\n",
    "        # 입력이 한글이면 먼저 영어로 번역\n",
    "        if not question.isascii():                \n",
    "            question_en = await ko2en(question)\n",
    "        else:\n",
    "            question_en = question\n",
    "\n",
    "        # 동의어 사전 기반 OR-확장 쿼리 생성\n",
    "        prompt = (\n",
    "            \"You are a pediatric-anesthesia search expert.\\n\"\n",
    "            \"Rewrite the following question into ONE concise English search query \"\n",
    "            \"optimised for a vector database.\\n\"\n",
    "            f\"Synonym dictionary: {json.dumps(PEDIATRIC_TERMS, ensure_ascii=False)}\\n\"\n",
    "            \"Guidelines:\\n\"\n",
    "            \"① Preserve key medical terms/abbreviations.\\n\"\n",
    "            \"② Expand synonyms with OR (e.g., neonatal OR newborn).\\n\"\n",
    "            \"③ Remove stop-words and unnecessary fillers.\\n\"\n",
    "            \"Return ONLY the search query.\"\n",
    "        )\n",
    "        if mode == \"improvement\":\n",
    "            prompt += \"\\n(Previous results were unsatisfactory. Try a different angle.)\"\n",
    "        elif mode not in {\"initial\", \"improvement\"}:\n",
    "            prompt += f\"\\n(User feedback: {mode} – please reflect it.)\"\n",
    "\n",
    "        prompt += f\"\\n\\nUser question: {question_en}\\n→\"\n",
    "\n",
    "        rewritten = (await self.llm.ainvoke(prompt)).content.strip()\n",
    "        return rewritten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5434653e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  다섯 번째 셀 — PubMed 전용 LLM 평가기 (핵심 3 지표)\n",
    "\"\"\"\n",
    "- Relevance(관련성) : 쿼리와 증거 간 주제 중복도\n",
    "- Faithfulness(사실 일치도) : 증거가 질문에 대한 답변을 지지하는지 여부\n",
    "- Completeness(질문 요소 충족도) : 증거가 질문의 모든 요소를 포함하는지 여부\n",
    "- G-Eval 원논문과 Confident-AI 해설에서 “Relevance가 인간 평가와 가장 높은 상관(ρ≈0.84)을 보였고 Faithfulness가 그다음(ρ≈0.81)”로 보고되어 두 지표를 0.4·0.35로 가장 크게 반영\n",
    "- RAGAS·Weaviate 가이드에 따르면 Completeness(답변이 쿼리의 모든 핵심 요소를 다루는지)는 품질에 기여하지만 상관계수는 앞 두 지표보다 낮아 보조 가중치(0.25)로 충분\n",
    "- 세 가중치 합을 1.0으로 두면 스코어 직관성이 유지되고, 0.6 이상(60 %)을 실무 통과선으로 삼는 최근 RAG 평가 사례들과 동일한 cut-off를 그대로 사용할 수 있다\n",
    "\"\"\"\n",
    "\n",
    "class LLMEvaluator:\n",
    "    \"\"\"Relevance(관련성), Faithfulness(사실 일치도), Completeness(질문 요소 충족도), 세 지표만 계산하는 LLM Judge\"\"\"\n",
    "    def __init__(self, model_name=\"gpt-4o-mini\", temperature=0.0):\n",
    "        self.judge = ChatOpenAI(model_name=model_name,\n",
    "                                temperature=temperature,\n",
    "                                max_tokens=500)\n",
    "\n",
    "    async def evaluate_search_results(self, query: str, docs: List[str]) -> Dict:\n",
    "        if not docs:\n",
    "            return self._default(\"no documents\")\n",
    "\n",
    "        previews = [d[:200].replace(\"\\n\", \" \") + (\"…\" if len(d) > 200 else \"\")\n",
    "                    for d in docs[:3]]\n",
    "\n",
    "        prompt = f\"\"\"\n",
    "            You are an independent evaluator for a PubMed-based RAG system.\n",
    "\n",
    "            Task ◂ Evaluate how well the retrieved abstracts answer the user query.\n",
    "            Metrics (0-1):\n",
    "            1. relevance    – topical overlap between query and abstracts.\n",
    "            2. faithfulness – factual alignment: does the evidence really support the answer?\n",
    "            3. completeness – does the evidence cover all key aspects of the question?\n",
    "\n",
    "            Return **one JSON**:\n",
    "\n",
    "            {{\n",
    "            \"relevance\": 0.0,\n",
    "            \"faithfulness\": 0.0,\n",
    "            \"completeness\": 0.0,\n",
    "            \"overall\": 0.0,\n",
    "            \"feedback\": \"\"\n",
    "            }}\n",
    "\n",
    "            overall = 0.4*relevance + 0.35*faithfulness + 0.25*completeness\n",
    "            User query: \\\"{query}\\\"\n",
    "            Evidence previews:\n",
    "            - {previews[0]}\n",
    "            - {previews[1] if len(previews)>1 else ''}\n",
    "            - {previews[2] if len(previews)>2 else ''}\n",
    "            \"\"\"\n",
    "        \n",
    "        try:\n",
    "            raw = (await self.judge.ainvoke(prompt)).content.strip()\n",
    "            js = re.search(r\"\\{.*\\}\", raw, re.S).group()\n",
    "            data = json.loads(js)\n",
    "            # 필드 누락 시 기본값 보정\n",
    "            for k in (\"relevance\", \"faithfulness\", \"completeness\"):\n",
    "                data.setdefault(k, 0.3)\n",
    "            # overall 미존재 시 가중합 계산\n",
    "            if \"overall\" not in data:\n",
    "                data[\"overall\"] = round(\n",
    "                    0.4*data[\"relevance\"] +\n",
    "                    0.35*data[\"faithfulness\"] +\n",
    "                    0.25*data[\"completeness\"], 3)\n",
    "            # 통과 기준(0.6)을 기본값으로 삽입\n",
    "            data.setdefault(\"recommended_threshold\", 0.6)\n",
    "            return data\n",
    "        except Exception as e:\n",
    "            return self._default(f\"parse error: {e}\")\n",
    "\n",
    "    # 모델이 어떤 지표를 빠뜨려도 0.3(보통 수준) 으로 채워 넣어 오류 방지\n",
    "    def _default(self, reason: str):\n",
    "        return dict(relevance=0.3, faithfulness=0.3, completeness=0.3,\n",
    "                    overall=0.3, feedback=reason, recommended_threshold=0.6)\n",
    "\n",
    "    # 종합 점수에 따른 재검색 여부 결정\n",
    "    async def should_retry_search(self, ev: Dict, turn: int, max_turn: int):\n",
    "        return ev.get(\"overall\", 0) < ev.get(\"recommended_threshold\", 0.6) \\\n",
    "               and turn < max_turn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f0a5f45f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "헬퍼 함수들 준비 완료\n"
     ]
    }
   ],
   "source": [
    "# 여섯 번째 셀: 헬퍼 함수들 (정렬 기준 제거)\n",
    "\n",
    "gpt_rewriter = GPTQueryRewriter()\n",
    "llm_evaluator = LLMEvaluator()\n",
    "\n",
    "# 한글 쿼리를 영어로 선번역 후 동의어 사전을 활용하여 혼합 방식 확장\n",
    "async def expand_medical_terms(query: str) -> str:\n",
    "    # 1. 한글이 포함된 경우 영어로 먼저 번역\n",
    "    if not query.isascii():\n",
    "        translated_query = await ko2en(query)\n",
    "        print(f\"영어 번역: {translated_query}\")\n",
    "    else:\n",
    "        translated_query = query\n",
    "    \n",
    "    # 2. 번역된 영어 쿼리에서 동의어 사전 매칭하여 혼합 방식 확장\n",
    "    expanded = translated_query\n",
    "    translated_lower = translated_query.lower()\n",
    "    \n",
    "    for preferred_term, synonyms in PEDIATRIC_TERMS.items():\n",
    "        # 선호 용어나 동의어가 번역된 쿼리에 포함되어 있는지 확인\n",
    "        if preferred_term in translated_lower or any(syn in translated_lower for syn in synonyms):\n",
    "            # 혼합 방식: 가장 관련성 높은 동의어 2-3개만 선택\n",
    "            relevant_synonyms = select_relevant_synonyms(synonyms, translated_query)\n",
    "            if relevant_synonyms:\n",
    "                synonym_phrase = \" OR \".join(relevant_synonyms)\n",
    "                expanded += f\" ({synonym_phrase})\"\n",
    "            break  # 첫 번째 매칭만 사용하여 중복 방지\n",
    "    \n",
    "    return expanded\n",
    "\n",
    "\n",
    "def select_relevant_synonyms(synonyms: List[str], original_query: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    동의어 리스트에서 가장 관련성 높은 2-3개 선택 (길이 기준 제거)\n",
    "    \"\"\"\n",
    "    filtered_synonyms = []\n",
    "    original_lower = original_query.lower()\n",
    "    \n",
    "    for syn in synonyms:\n",
    "        syn_lower = syn.lower()\n",
    "        # 제외 조건들\n",
    "        if (len(syn) < 3 or  # 너무 짧은 용어\n",
    "            len(syn) > 20 or  # 너무 긴 용어\n",
    "            syn_lower in original_lower or  # 이미 원래 쿼리에 포함된 용어\n",
    "            syn_lower in ['child', 'children', 'pediatric', 'paediatric']):  # 너무 일반적인 용어\n",
    "            continue\n",
    "        filtered_synonyms.append(syn)\n",
    "    \n",
    "    # 최대 3개까지만 선택 (순서 유지)\n",
    "    return filtered_synonyms[:3]\n",
    "\n",
    "async def multi_strategy_query_expansion(original: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    하이브리드 전략으로 쿼리를 확장 (GPT 재작성 + 동의어 사전 활용)\n",
    "    \"\"\"\n",
    "    strategies = []\n",
    "    \n",
    "    # 1. 원본 쿼리 (한글이면 영어로 번역)\n",
    "    if not original.isascii():\n",
    "        translated_original = await ko2en(original)\n",
    "        strategies.append(translated_original)\n",
    "        print(f\"원본 번역: {translated_original}\")\n",
    "    else:\n",
    "        strategies.append(original)\n",
    "    \n",
    "    try:\n",
    "        # 2. GPT 재작성 (의미적 이해 기반 쿼리 최적화)\n",
    "        gpt_rewritten = await gpt_rewriter.rewrite(original, \"initial\")\n",
    "        strategies.append(gpt_rewritten)\n",
    "        print(f\"GPT 재작성: {gpt_rewritten}\")\n",
    "    except Exception as e:\n",
    "        print(f\"GPT 재작성 실패: {e}\")\n",
    "    \n",
    "    # 3. 동의어 사전 기반 확장\n",
    "    medical_expanded = await expand_medical_terms(original)\n",
    "    strategies.append(medical_expanded)\n",
    "    print(f\"의료 용어 확장: {medical_expanded}\")\n",
    "    \n",
    "    # 4. 고급 중복 제거 (의미적 유사성 고려)\n",
    "    unique_strategies = []\n",
    "    for strategy in strategies:\n",
    "        if strategy and strategy.strip():\n",
    "            is_duplicate = False\n",
    "            for existing in unique_strategies:\n",
    "                if (strategy.lower() == existing.lower() or \n",
    "                    strategy.lower() in existing.lower() or \n",
    "                    existing.lower() in strategy.lower()):\n",
    "                    if len(strategy) > len(existing):\n",
    "                        unique_strategies.remove(existing)\n",
    "                        unique_strategies.append(strategy)\n",
    "                    is_duplicate = True\n",
    "                    break\n",
    "            \n",
    "            if not is_duplicate:\n",
    "                unique_strategies.append(strategy)\n",
    "    \n",
    "    print(f\"최종 전략 수: {len(unique_strategies)}\")\n",
    "    return unique_strategies\n",
    "\n",
    "# 중복 문서 제거 및 랭킹\n",
    "def remove_duplicates_and_rank(docs: List) -> List:\n",
    "    seen = set()\n",
    "    unique_docs = []\n",
    "    \n",
    "    for doc in docs:\n",
    "        content_hash = hash(doc.page_content[:200])\n",
    "        if content_hash not in seen:\n",
    "            seen.add(content_hash)\n",
    "            unique_docs.append(doc)\n",
    "    \n",
    "    return unique_docs\n",
    "\n",
    "# LLMEvaluator를 사용하여 실제 검색 품질 기반으로 최고의 쿼리를 선택\n",
    "async def select_best_query_with_llm_evaluator(query_variants: List[str], original_question: str) -> tuple[str, Dict]:\n",
    "    if not query_variants:\n",
    "        return original_question, {}\n",
    "    \n",
    "    if len(query_variants) == 1:\n",
    "        docs = retriever.invoke(query_variants[0])\n",
    "        docs_text = [doc.page_content for doc in docs]\n",
    "        evaluation = await llm_evaluator.evaluate_search_results(query_variants[0], docs_text)\n",
    "        return query_variants[0], evaluation\n",
    "    \n",
    "    print(f\"{len(query_variants)}개 쿼리 변형에 대해 검색 품질 평가 시작\")\n",
    "    \n",
    "    best_query = query_variants[0]\n",
    "    best_score = 0\n",
    "    best_evaluation = {}\n",
    "    evaluations = []\n",
    "    \n",
    "    for i, query in enumerate(query_variants):\n",
    "        try:\n",
    "            print(f\"쿼리 {i+1}/{len(query_variants)}: {query}\")\n",
    "            docs = retriever.invoke(query)\n",
    "            docs_text = [doc.page_content for doc in docs]\n",
    "            evaluation = await llm_evaluator.evaluate_search_results(query, docs_text)\n",
    "            evaluations.append((query, evaluation))\n",
    "            overall_score = evaluation.get(\"overall\", 0)\n",
    "            print(f\"평가 점수: {overall_score:.3f}\")\n",
    "            print(f\"세부점수 - 관련성:{evaluation.get('relevance', 0):.2f} \"\n",
    "                  f\"일치도:{evaluation.get('faithfulness', 0):.2f} \"\n",
    "                  f\"완성도:{evaluation.get('completeness', 0):.2f}\")\n",
    "            \n",
    "            if overall_score > best_score:\n",
    "                best_score = overall_score\n",
    "                best_query = query\n",
    "                best_evaluation = evaluation\n",
    "                print(f\"현재 최고 쿼리 업데이트!\")\n",
    "        except Exception as e:\n",
    "            print(f\"쿼리 평가 중 오류: {e}\")\n",
    "            default_eval = {\n",
    "                \"relevance\": 0.3,\n",
    "                \"faithfulness\": 0.3, \n",
    "                \"completeness\": 0.3,\n",
    "                \"overall\": 0.3,\n",
    "                \"feedback\": f\"평가 오류: {str(e)[:50]}\",\n",
    "                \"recommended_threshold\": 0.6\n",
    "            }\n",
    "            evaluations.append((query, default_eval))\n",
    "    \n",
    "    print(f\"최종 선택된 쿼리: {best_query}\")\n",
    "    print(f\"최고 점수: {best_score:.3f}\")\n",
    "    print(f\"선택 근거: {best_evaluation.get('feedback', '평가 완료')}\")\n",
    "    \n",
    "    print(\"\\n전체 쿼리 평가 결과:\")\n",
    "    for query, eval_result in evaluations:\n",
    "        score = eval_result.get(\"overall\", 0)\n",
    "        print(f\"  {score:.3f} | {query}\")\n",
    "    \n",
    "    return best_query, best_evaluation\n",
    "\n",
    "async def select_multiple_best_queries_with_evaluation(query_variants: List[str], original_question: str, top_k: int = 2) -> List[tuple[str, Dict]]:\n",
    "    \"\"\"\n",
    "    LLMEvaluator 기반으로 상위 K개의 쿼리와 평가 결과를 반환\n",
    "    \"\"\"\n",
    "    if not query_variants or len(query_variants) <= top_k:\n",
    "        results = []\n",
    "        for query in query_variants or [original_question]:\n",
    "            try:\n",
    "                docs = retriever.invoke(query)\n",
    "                docs_text = [doc.page_content for doc in docs]\n",
    "                evaluation = await llm_evaluator.evaluate_search_results(query, docs_text)\n",
    "                results.append((query, evaluation))\n",
    "            except Exception as e:\n",
    "                default_eval = {\"overall\": 0.3, \"feedback\": f\"오류: {e}\"}\n",
    "                results.append((query, default_evaluator))\n",
    "        return results\n",
    "    \n",
    "    print(f\"상위 {top_k}개 쿼리 선택을 위한 전체 평가 시작\")\n",
    "    \n",
    "    scored_queries = []\n",
    "    for query in query_variants:\n",
    "        try:\n",
    "            docs = retriever.invoke(query)\n",
    "            docs_text = [doc.page_content for doc in docs]\n",
    "            evaluation = await llm_evaluator.evaluate_search_results(query, docs_text)\n",
    "            overall_score = evaluation.get(\"overall\", 0)\n",
    "            scored_queries.append((query, evaluation, overall_score))\n",
    "        except Exception as e:\n",
    "            default_eval = {\n",
    "                \"overall\": 0.3, \n",
    "                \"feedback\": f\"평가 오류: {str(e)[:50]}\",\n",
    "                \"relevance\": 0.3, \"faithfulness\": 0.3, \"completeness\": 0.3\n",
    "            }\n",
    "            scored_queries.append((query, default_eval, 0.3))\n",
    "    \n",
    "    scored_queries.sort(key=lambda x: x[2], reverse=True)\n",
    "    selected = [(query, evaluation) for query, evaluation, score in scored_queries[:top_k]]\n",
    "    \n",
    "    print(f\"상위 {top_k}개 쿼리 선택 완료:\")\n",
    "    for i, (query, evaluation) in enumerate(selected):\n",
    "        print(f\"  {i+1}. {evaluation.get('overall', 0):.3f} | {query}\")\n",
    "    \n",
    "    return selected\n",
    "\n",
    "print(\"헬퍼 함수들 준비 완료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "06ca8456",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt_rewriter 상태: GPTQueryRewriter\n",
      "llm_evaluator 상태: LLMEvaluator\n",
      "retriever 상태: ContextualCompressionRetriever\n",
      "chat_model 상태: ChatOpenAI\n",
      "State 및 모델 설정 완료\n"
     ]
    }
   ],
   "source": [
    "# 일곱 번째 셀: State 및 모델 설정\n",
    "MAX_RETRY = 4\n",
    "\n",
    "class ChatbotState(TypedDict):\n",
    "    original_question: Annotated[str, \"Original_Question\"]\n",
    "    current_query: Annotated[str, \"Current_Query\"]\n",
    "    query_variants: Annotated[List[str], \"Query_Variants\"]\n",
    "    vector_documents: Annotated[str, \"Vector_Documents\"]\n",
    "    graph_db_context: Annotated[str, \"Graph_DB_Context\"]\n",
    "    llm_evaluation: Annotated[Dict, \"LLM_Evaluation\"]\n",
    "    loop_cnt: Annotated[int, \"Loop_Count\"]\n",
    "    final_answer: Annotated[str, \"Final_Answer\"]\n",
    "    messages: Annotated[List, add_messages]\n",
    "\n",
    "memory = MemorySaver()\n",
    "\n",
    "LLM_SYSTEM_PROMPT = \"\"\"# 소아마취학 전문가 AI 시스템\n",
    "\n",
    "## 역할 정의\n",
    "당신은 소아마취학 전문가 AI로서, 임상 의사결정 지원 시스템(Clinical Decision Support System)의 역할을 합니다. \n",
    "신생아부터 청소년까지의 소아 환자 마취 관리에 대한 전문적이고 근거 기반의 임상 정보를 제공합니다.\n",
    "\n",
    "## 의사소통 원칙\n",
    "- 명확하고 실용적인 정보 제공\n",
    "- 근거 기반 권고사항 명시\n",
    "- 불확실한 경우 추가 전문의 상담 권고\n",
    "- 안전성을 최우선으로 고려\n",
    "\n",
    "## 컨텍스트 정보\n",
    "\n",
    "### Vector DB 검색 결과\n",
    "{VectorDB}\n",
    "\n",
    "### 그래프 DB 환자 정보 (해당하는 경우)\n",
    "{GraphDB}\n",
    "\n",
    "## 사용자 질문\n",
    "{question}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "chat_model = ChatOpenAI(temperature=0.2, model_name=\"gpt-4o-mini\")\n",
    "\n",
    "# 전역 변수들이 제대로 생성되었는지 확인\n",
    "try:\n",
    "    print(f\"gpt_rewriter 상태: {type(gpt_rewriter).__name__}\")\n",
    "    print(f\"llm_evaluator 상태: {type(llm_evaluator).__name__}\")\n",
    "    print(f\"retriever 상태: {type(retriever).__name__}\")\n",
    "    print(f\"chat_model 상태: {type(chat_model).__name__}\")\n",
    "except NameError as e:\n",
    "    print(f\"변수 정의 오류: {e}\")\n",
    "\n",
    "print(\"State 및 모델 설정 완료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "875b86a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "개선된 LLMEvaluator 기반 Best Query 선택 로직 적용 완료!\n"
     ]
    }
   ],
   "source": [
    "# 개선된 gpt_query_rewriter_node (LLMEvaluator 기반 Best Query 선택)\n",
    "async def gpt_query_rewriter_node(state: ChatbotState):\n",
    "    \"\"\"GPT를 이용한 쿼리 재작성 노드 (LLMEvaluator 기반 Best Query 선택)\"\"\"\n",
    "    loop_cnt = state.get(\"loop_cnt\", 0)\n",
    "    print(f\"쿼리 재작성 노드 - 시도 {loop_cnt + 1}\")\n",
    "    \n",
    "    try:\n",
    "        if loop_cnt == 0:\n",
    "            # 첫 번째 시도: 다중 전략 쿼리 확장\n",
    "            query_variants = await multi_strategy_query_expansion(state[\"original_question\"])\n",
    "            print(f\"생성된 쿼리 변형들: {query_variants}\")\n",
    "            \n",
    "            # LLMEvaluator 기반 best query 선택\n",
    "            best_query, best_evaluation = await select_best_query_with_llm_evaluator(\n",
    "                query_variants, \n",
    "                state[\"original_question\"]\n",
    "            )\n",
    "            print(f\"LLM 평가 기반으로 선택된 최고 쿼리: {best_query}\")\n",
    "            print(f\"선택된 쿼리의 점수: {best_evaluation.get('overall', 0):.3f}\")\n",
    "            \n",
    "        elif loop_cnt == 1:\n",
    "            # 두 번째 시도: 기존 변형 중 다른 쿼리 시도\n",
    "            existing_variants = state.get(\"query_variants\", [state[\"original_question\"]])\n",
    "            if len(existing_variants) > 1:\n",
    "                # 상위 2개 쿼리 중 두 번째 선택\n",
    "                selected_queries = await select_multiple_best_queries_with_evaluation(\n",
    "                    existing_variants, state[\"original_question\"], top_k=2\n",
    "                )\n",
    "                if len(selected_queries) > 1:\n",
    "                    best_query, best_evaluation = selected_queries[1]  # 두 번째 최고 쿼리\n",
    "                    print(f\"두 번째 최고 쿼리 선택: {best_query}\")\n",
    "                else:\n",
    "                    best_query, best_evaluation = selected_queries[0]\n",
    "                    print(f\"첫 번째 쿼리 재사용: {best_query}\")\n",
    "            else:\n",
    "                best_query = existing_variants[0] if existing_variants else state[\"original_question\"]\n",
    "                print(f\"기존 쿼리 사용: {best_query}\")\n",
    "                \n",
    "        elif loop_cnt == 2:\n",
    "            # 세 번째 시도: 다양성 기반 선택 또는 새로운 재작성\n",
    "            existing_variants = state.get(\"query_variants\", [state[\"original_question\"]])\n",
    "            try:\n",
    "                best_query = await gpt_rewriter.rewrite(state['original_question'], \"improvement\")\n",
    "                print(f\"개선 기반 재작성: {best_query}\")\n",
    "            except:\n",
    "                best_query = existing_variants[-1] if existing_variants else state[\"original_question\"]\n",
    "                print(f\"마지막 변형 사용: {best_query}\")\n",
    "                \n",
    "        else:\n",
    "            # 마지막 시도: 원본 질문 사용\n",
    "            best_query = state[\"original_question\"]\n",
    "            print(f\"원본 질문 사용: {best_query}\")\n",
    "        \n",
    "        return ChatbotState(\n",
    "            current_query=best_query,\n",
    "            query_variants=await multi_strategy_query_expansion(state[\"original_question\"]) if loop_cnt == 0 else state.get(\"query_variants\", []),\n",
    "            loop_cnt=loop_cnt + 1\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"쿼리 재작성 중 오류: {e}\")\n",
    "        # 오류 시 원본 질문 사용\n",
    "        return ChatbotState(\n",
    "            current_query=state[\"original_question\"],\n",
    "            query_variants=[state[\"original_question\"]],\n",
    "            loop_cnt=loop_cnt + 1\n",
    "        )\n",
    "\n",
    "print(\"개선된 LLMEvaluator 기반 Best Query 선택 로직 적용 완료!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0ba1ab55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangGraph 노드들 준비 완료\n"
     ]
    }
   ],
   "source": [
    "# 여덟 번째 셀: LangGraph 노드들\n",
    "async def gpt_query_rewriter_node(state: ChatbotState):\n",
    "    \"\"\"GPT를 이용한 쿼리 재작성 노드 (LLMEvaluator 기반 Best Query 선택)\"\"\"\n",
    "    loop_cnt = state.get(\"loop_cnt\", 0)\n",
    "    try:\n",
    "        if loop_cnt == 0:\n",
    "            query_variants = await multi_strategy_query_expansion(state[\"original_question\"])\n",
    "            best_query, best_evaluation = await select_best_query_with_llm_evaluator(\n",
    "                query_variants, state[\"original_question\"]\n",
    "            )\n",
    "        elif loop_cnt == 1:\n",
    "            existing_variants = state.get(\"query_variants\", [state[\"original_question\"]])\n",
    "            selected = await select_multiple_best_queries_with_evaluation(\n",
    "                existing_variants, state[\"original_question\"], top_k=2\n",
    "            )\n",
    "            best_query = selected[1][0] if len(selected) > 1 else selected[0][0]\n",
    "        elif loop_cnt == 2:\n",
    "            try:\n",
    "                best_query = await gpt_rewriter.rewrite(state['original_question'], \"improvement\")\n",
    "            except:\n",
    "                best_query = state[\"query_variants\"][-1]\n",
    "        else:\n",
    "            best_query = state[\"original_question\"]\n",
    "        return ChatbotState(\n",
    "            current_query=best_query,\n",
    "            query_variants=(query_variants if loop_cnt == 0 else state.get(\"query_variants\", [])),\n",
    "            loop_cnt=loop_cnt + 1\n",
    "        )\n",
    "    except Exception as e:\n",
    "        return ChatbotState(\n",
    "            current_query=state[\"original_question\"],\n",
    "            query_variants=[state[\"original_question\"]],\n",
    "            loop_cnt=loop_cnt+1\n",
    "        )\n",
    "\n",
    "async def ensemble_search_node(state: ChatbotState):\n",
    "    \"\"\"Vector DB에서 문서 검색을 수행하는 노드\"\"\"\n",
    "    current_query = state.get(\"current_query\")\n",
    "    docs = retriever.invoke(current_query)\n",
    "    result_text = \"\\n\".join([d.page_content for d in docs])\n",
    "    return ChatbotState(vector_documents=result_text)\n",
    "\n",
    "async def llm_evaluation_node(state: ChatbotState):\n",
    "    \"\"\"검색 결과의 품질을 평가하는 노드\"\"\"\n",
    "    docs_list = [d for d in state.get(\"vector_documents\", \"\").split(\"\\n\") if d]\n",
    "    if not docs_list:\n",
    "        return ChatbotState(llm_evaluation=LLMEvaluator()._default(\"no docs\"))\n",
    "    evaluation = await llm_evaluator.evaluate_search_results(state.get(\"current_query\"), docs_list)\n",
    "    for field in [\"relevance\",\"faithfulness\",\"completeness\",\"overall\",\"feedback\",\"recommended_threshold\"]:\n",
    "        evaluation.setdefault(field, 0.3 if field!=\"feedback\" else \"missing\")\n",
    "    return ChatbotState(llm_evaluation=evaluation)\n",
    "\n",
    "async def merge_outputs(state: ChatbotState):\n",
    "    \"\"\"검색 결과와 평가 피드백을 바탕으로 최종 답변을 생성하는 노드\"\"\"\n",
    "    eval = state.get(\"llm_evaluation\", {})\n",
    "    overall = eval.get(\"overall\", 0)\n",
    "    threshold = eval.get(\"recommended_threshold\", 0.6)\n",
    "    # 평가 점수 미달 시 일반화된 안내 메시지\n",
    "    if overall < threshold:\n",
    "        fallback = (\n",
    "            \"현재 문헌 검색으로는 질문하신 사항에 정확히 답변드리기 어렵습니다. \"\n",
    "            \"유사한 일반 소아 KMS 치료 지침을 참고하시거나, 질문을 구체화하여 다시 문의해 주세요.\"\n",
    "        )\n",
    "        return ChatbotState(final_answer=fallback, messages=[(\"assistant\", fallback)])\n",
    "    # 충분한 자료 확보 시 LLM을 이용해 상세 답변 생성\n",
    "    context = state.get(\"vector_documents\", \"\")\n",
    "    prompt = LLM_SYSTEM_PROMPT.format(\n",
    "        VectorDB=context,\n",
    "        GraphDB=state.get(\"graph_db_context\", \"\"),\n",
    "        question=state.get(\"original_question\")\n",
    "    )\n",
    "    response = chat_model.invoke(prompt)\n",
    "    answer = response.content or \"죄송합니다. 답변을 생성할 수 없습니다.\"\n",
    "    return ChatbotState(final_answer=answer, messages=[(\"assistant\", answer)])\n",
    "\n",
    "print(\"LangGraph 노드들 준비 완료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "74a5279c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangGraph 빌드 완료\n"
     ]
    }
   ],
   "source": [
    "# 아홉 번째 셀: LangGraph 빌드\n",
    "sg = StateGraph(ChatbotState)\n",
    "sg.add_node(\"gpt_query_rewriter\", gpt_query_rewriter_node)\n",
    "sg.add_node(\"ensemble_search\", ensemble_search_node)\n",
    "sg.add_node(\"llm_evaluation\", llm_evaluation_node)\n",
    "sg.add_node(\"merge_outputs\", merge_outputs)\n",
    "\n",
    "sg.add_edge(START, \"gpt_query_rewriter\")\n",
    "sg.add_edge(\"gpt_query_rewriter\", \"ensemble_search\")\n",
    "sg.add_edge(\"ensemble_search\", \"llm_evaluation\")\n",
    "\n",
    "async def _llm_based_route(state: ChatbotState):\n",
    "    \"\"\"평가 결과에 따른 라우팅 결정 함수\"\"\"\n",
    "    evaluation = state.get(\"llm_evaluation\", {})\n",
    "    loop_cnt = state.get(\"loop_cnt\", 0)\n",
    "    \n",
    "    # 평가 정보 로깅\n",
    "    overall_score = evaluation.get(\"overall\", 0)\n",
    "    threshold = evaluation.get(\"recommended_threshold\", 0.6)\n",
    "    print(f\"라우팅 평가: 점수 {overall_score:.3f} vs 임계값 {threshold:.3f}\")\n",
    "    print(f\"현재 시도 횟수: {loop_cnt}/{MAX_RETRY}\")\n",
    "    \n",
    "    try:\n",
    "        should_retry = await llm_evaluator.should_retry_search(evaluation, loop_cnt, MAX_RETRY)\n",
    "        result = \"retry_rewrite\" if should_retry else \"generate_answer\"\n",
    "        \n",
    "        if should_retry:\n",
    "            print(f\"품질 부족으로 재시도: {result}\")\n",
    "            print(f\"피드백: {evaluation.get('feedback', '피드백 없음')}\")\n",
    "        else:\n",
    "            print(f\"품질 충족 또는 최대 시도 도달: {result}\")\n",
    "        \n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"라우팅 결정 중 오류: {e}\")\n",
    "        # 오류 시 답변 생성으로 진행\n",
    "        return \"generate_answer\"\n",
    "\n",
    "sg.add_conditional_edges(\n",
    "    \"llm_evaluation\",\n",
    "    _llm_based_route,\n",
    "    {\n",
    "        \"retry_rewrite\": \"gpt_query_rewriter\",\n",
    "        \"generate_answer\": \"merge_outputs\",\n",
    "    },\n",
    ")\n",
    "\n",
    "sg.add_edge(\"merge_outputs\", END)\n",
    "\n",
    "graph = sg.compile(checkpointer=memory)\n",
    "\n",
    "print(\"LangGraph 빌드 완료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f0414db3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "실행 함수 준비 완료\n"
     ]
    }
   ],
   "source": [
    "# 열 번째 셀: 실행 함수\n",
    "async def run_chatbot(question: str, show_details: bool = True, graph_db_context: str = \"\"):\n",
    "    \"\"\"Jupyter용 챗봇 실행 함수\"\"\"\n",
    "    config = RunnableConfig(configurable={\"thread_id\": 1})\n",
    "    \n",
    "    init_state = {\n",
    "        \"original_question\": question,\n",
    "        \"current_query\": \"\",\n",
    "        \"query_variants\": [],\n",
    "        \"vector_documents\": \"\",\n",
    "        \"graph_db_context\": graph_db_context,\n",
    "        \"llm_evaluation\": {},\n",
    "        \"loop_cnt\": 0,\n",
    "        \"final_answer\": \"\",\n",
    "        \"messages\": [],\n",
    "    }\n",
    "    \n",
    "    print(f\"원본 질문: {question}\")\n",
    "    print(\"=\" * 100)\n",
    "    \n",
    "    final_result = None\n",
    "    \n",
    "    async for event in graph.astream(init_state, config=config):\n",
    "        for node_name, node_state in event.items():\n",
    "            print(f\"\\n현재 노드: {node_name}\")\n",
    "            \n",
    "            if show_details:\n",
    "                if node_name == \"gpt_query_rewriter\":\n",
    "                    print(f\"GPT 재작성된 쿼리 (시도 {node_state['loop_cnt']}): {node_state['current_query']}\")\n",
    "                    \n",
    "                elif node_name == \"llm_evaluation\":\n",
    "                    evaluation = node_state['llm_evaluation']\n",
    "                    print(f\"LLM 평가 결과 (시도 {node_state.get('loop_cnt', 0)}):\")\n",
    "                    print(f\"   관련성: {evaluation.get('relevance', 0):.3f}\")\n",
    "                    print(f\"   사실 일치도: {evaluation.get('faithfulness', 0):.3f}\")\n",
    "                    print(f\"   정보 완성도: {evaluation.get('completeness', 0):.3f}\")\n",
    "                    print(f\"   종합 점수: {evaluation.get('overall', 0):.3f}\")\n",
    "                    print(f\"   권장 임계치: {evaluation.get('recommended_threshold', 0):.3f}\")\n",
    "                    \n",
    "                    if evaluation.get('overall', 0) < evaluation.get('recommended_threshold', 0.6):\n",
    "                        print(\"   점수가 낮아 쿼리를 다시 재작성합니다.\")\n",
    "                        print(f\"   피드백: {evaluation.get('feedback', '')}\")\n",
    "                    else:\n",
    "                        print(\"   점수가 충분합니다.\")\n",
    "                    print(\"-\" * 80)\n",
    "                    \n",
    "                elif node_name == \"merge_outputs\":\n",
    "                    print(\"최종 답변:\")\n",
    "                    print(node_state['final_answer'])\n",
    "                    final_result = node_state\n",
    "    \n",
    "    return final_result\n",
    "\n",
    "print(\"실행 함수 준비 완료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ecf00f1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLMEvaluator 기반 그래프 빌드 완료\n"
     ]
    }
   ],
   "source": [
    "# 그래프 재빌드 (업데이트된 함수들 적용)\n",
    "try:\n",
    "    del graph  # 기존 그래프 삭제\n",
    "except:\n",
    "    pass\n",
    "\n",
    "sg = StateGraph(ChatbotState)\n",
    "sg.add_node(\"gpt_query_rewriter\", gpt_query_rewriter_node)\n",
    "sg.add_node(\"ensemble_search\", ensemble_search_node)\n",
    "sg.add_node(\"llm_evaluation\", llm_evaluation_node)\n",
    "sg.add_node(\"merge_outputs\", merge_outputs)\n",
    "\n",
    "sg.add_edge(START, \"gpt_query_rewriter\")\n",
    "sg.add_edge(\"gpt_query_rewriter\", \"ensemble_search\")\n",
    "sg.add_edge(\"ensemble_search\", \"llm_evaluation\")\n",
    "\n",
    "async def _llm_based_route(state: ChatbotState):\n",
    "    \"\"\"평가 결과에 따른 라우팅 결정 함수\"\"\"\n",
    "    evaluation = state.get(\"llm_evaluation\", {})\n",
    "    loop_cnt = state.get(\"loop_cnt\", 0)\n",
    "    \n",
    "    # 평가 정보 로깅\n",
    "    overall_score = evaluation.get(\"overall\", 0)\n",
    "    threshold = evaluation.get(\"recommended_threshold\", 0.6)\n",
    "    print(f\"라우팅 평가: 점수 {overall_score:.3f} vs 임계값 {threshold:.3f}\")\n",
    "    print(f\"현재 시도 횟수: {loop_cnt}/{MAX_RETRY}\")\n",
    "    \n",
    "    try:\n",
    "        should_retry = await llm_evaluator.should_retry_search(evaluation, loop_cnt, MAX_RETRY)\n",
    "        result = \"retry_rewrite\" if should_retry else \"generate_answer\"\n",
    "        \n",
    "        if should_retry:\n",
    "            print(f\"품질 부족으로 재시도: {result}\")\n",
    "            print(f\"피드백: {evaluation.get('feedback', '피드백 없음')}\")\n",
    "        else:\n",
    "            print(f\"품질 충족 또는 최대 시도 도달: {result}\")\n",
    "        \n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"라우팅 결정 중 오류: {e}\")\n",
    "        # 오류 시 답변 생성으로 진행\n",
    "        return \"generate_answer\"\n",
    "\n",
    "sg.add_conditional_edges(\n",
    "    \"llm_evaluation\",\n",
    "    _llm_based_route,\n",
    "    {\n",
    "        \"retry_rewrite\": \"gpt_query_rewriter\",\n",
    "        \"generate_answer\": \"merge_outputs\",\n",
    "    },\n",
    ")\n",
    "\n",
    "sg.add_edge(\"merge_outputs\", END)\n",
    "\n",
    "graph = sg.compile(checkpointer=memory)\n",
    "\n",
    "print(\"LLMEvaluator 기반 그래프 빌드 완료\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8095bbb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원본 질문: 3세 Kasabach-Merritt Syndrome 환자의 일반적 치료법은?\n",
      "====================================================================================================\n",
      "원본 번역: What is the general treatment method for a 3-year-old patient with Kasabach-Merritt Syndrome?\n",
      "GPT 재작성: treatment OR management AND 3-year-old OR pediatric AND Kasabach-Merritt Syndrome\n",
      "영어 번역: What is the general treatment for a 3-year-old patient with Kasabach-Merritt Syndrome?\n",
      "의료 용어 확장: What is the general treatment for a 3-year-old patient with Kasabach-Merritt Syndrome?\n",
      "최종 전략 수: 3\n",
      "3개 쿼리 변형에 대해 검색 품질 평가 시작\n",
      "쿼리 1/3: What is the general treatment method for a 3-year-old patient with Kasabach-Merritt Syndrome?\n",
      "평가 점수: 0.485\n",
      "세부점수 - 관련성:0.50 일치도:0.60 완성도:0.40\n",
      "현재 최고 쿼리 업데이트!\n",
      "쿼리 2/3: treatment OR management AND 3-year-old OR pediatric AND Kasabach-Merritt Syndrome\n",
      "평가 점수: 0.485\n",
      "세부점수 - 관련성:0.50 일치도:0.70 완성도:0.40\n",
      "쿼리 3/3: What is the general treatment for a 3-year-old patient with Kasabach-Merritt Syndrome?\n",
      "평가 점수: 0.485\n",
      "세부점수 - 관련성:0.50 일치도:0.60 완성도:0.40\n",
      "최종 선택된 쿼리: What is the general treatment method for a 3-year-old patient with Kasabach-Merritt Syndrome?\n",
      "최고 점수: 0.485\n",
      "선택 근거: The retrieved abstracts provide some relevant information regarding treatment methods for Kasabach-Merritt Syndrome, particularly in infants and specific cases. However, they do not comprehensively address the general treatment methods for a 3-year-old patient, which is the focus of the user query. The abstracts mention specific treatments and experiences but lack a broader overview of treatment options suitable for the age group specified.\n",
      "\n",
      "전체 쿼리 평가 결과:\n",
      "  0.485 | What is the general treatment method for a 3-year-old patient with Kasabach-Merritt Syndrome?\n",
      "  0.485 | treatment OR management AND 3-year-old OR pediatric AND Kasabach-Merritt Syndrome\n",
      "  0.485 | What is the general treatment for a 3-year-old patient with Kasabach-Merritt Syndrome?\n",
      "\n",
      "현재 노드: gpt_query_rewriter\n",
      "GPT 재작성된 쿼리 (시도 1): What is the general treatment method for a 3-year-old patient with Kasabach-Merritt Syndrome?\n",
      "\n",
      "현재 노드: ensemble_search\n",
      "라우팅 평가: 점수 0.190 vs 임계값 0.600\n",
      "현재 시도 횟수: 1/4\n",
      "품질 부족으로 재시도: retry_rewrite\n",
      "피드백: The retrieved abstracts provide some information related to Kasabach-Merritt Syndrome, but they do not specifically address the treatment methods for a 3-year-old patient. The relevance is low as the abstracts focus on adult cases and specific surgical interventions that may not apply to pediatric patients. The faithfulness is moderate as the information about KMS is accurate, but it does not support the specific query about treatment methods. Completeness is low since key aspects of treatment for a young child are not covered.\n",
      "\n",
      "현재 노드: llm_evaluation\n",
      "LLM 평가 결과 (시도 0):\n",
      "   관련성: 0.200\n",
      "   사실 일치도: 0.300\n",
      "   정보 완성도: 0.100\n",
      "   종합 점수: 0.190\n",
      "   권장 임계치: 0.600\n",
      "   점수가 낮아 쿼리를 다시 재작성합니다.\n",
      "   피드백: The retrieved abstracts provide some information related to Kasabach-Merritt Syndrome, but they do not specifically address the treatment methods for a 3-year-old patient. The relevance is low as the abstracts focus on adult cases and specific surgical interventions that may not apply to pediatric patients. The faithfulness is moderate as the information about KMS is accurate, but it does not support the specific query about treatment methods. Completeness is low since key aspects of treatment for a young child are not covered.\n",
      "--------------------------------------------------------------------------------\n",
      "상위 2개 쿼리 선택을 위한 전체 평가 시작\n",
      "상위 2개 쿼리 선택 완료:\n",
      "  1. 0.485 | What is the general treatment method for a 3-year-old patient with Kasabach-Merritt Syndrome?\n",
      "  2. 0.485 | treatment OR management AND 3-year-old OR pediatric AND Kasabach-Merritt Syndrome\n",
      "\n",
      "현재 노드: gpt_query_rewriter\n",
      "GPT 재작성된 쿼리 (시도 2): treatment OR management AND 3-year-old OR pediatric AND Kasabach-Merritt Syndrome\n",
      "\n",
      "현재 노드: ensemble_search\n",
      "라우팅 평가: 점수 0.385 vs 임계값 0.600\n",
      "현재 시도 횟수: 2/4\n",
      "품질 부족으로 재시도: retry_rewrite\n",
      "피드백: The retrieved abstracts mention Kasabach-Merritt Syndrome (KMS) but primarily focus on adult cases and specific surgical interventions rather than treatment or management strategies for pediatric patients or 3-year-olds. The relevance is moderate due to the mention of KMS, but the focus on adult cases limits its applicability to the query. Faithfulness is somewhat supported as the abstracts discuss KMS accurately, but the lack of pediatric context affects completeness significantly.\n",
      "\n",
      "현재 노드: llm_evaluation\n",
      "LLM 평가 결과 (시도 0):\n",
      "   관련성: 0.300\n",
      "   사실 일치도: 0.500\n",
      "   정보 완성도: 0.200\n",
      "   종합 점수: 0.385\n",
      "   권장 임계치: 0.600\n",
      "   점수가 낮아 쿼리를 다시 재작성합니다.\n",
      "   피드백: The retrieved abstracts mention Kasabach-Merritt Syndrome (KMS) but primarily focus on adult cases and specific surgical interventions rather than treatment or management strategies for pediatric patients or 3-year-olds. The relevance is moderate due to the mention of KMS, but the focus on adult cases limits its applicability to the query. Faithfulness is somewhat supported as the abstracts discuss KMS accurately, but the lack of pediatric context affects completeness significantly.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "현재 노드: gpt_query_rewriter\n",
      "GPT 재작성된 쿼리 (시도 3): treatment method for 3-year-old patient with Kasabach-Merritt Syndrome\n",
      "\n",
      "현재 노드: ensemble_search\n",
      "라우팅 평가: 점수 0.190 vs 임계값 0.600\n",
      "현재 시도 횟수: 3/4\n",
      "품질 부족으로 재시도: retry_rewrite\n",
      "피드백: The retrieved abstracts provide some information related to Kasabach-Merritt Syndrome, but they do not specifically address treatment methods for a 3-year-old patient. The relevance is low due to the lack of direct applicability to the age group and condition specified in the query. Faithfulness is moderate as the abstracts mention KMS and its characteristics, but they do not provide concrete treatment options. Completeness is low as key aspects of treatment for a pediatric patient are not covered.\n",
      "\n",
      "현재 노드: llm_evaluation\n",
      "LLM 평가 결과 (시도 0):\n",
      "   관련성: 0.200\n",
      "   사실 일치도: 0.300\n",
      "   정보 완성도: 0.100\n",
      "   종합 점수: 0.190\n",
      "   권장 임계치: 0.600\n",
      "   점수가 낮아 쿼리를 다시 재작성합니다.\n",
      "   피드백: The retrieved abstracts provide some information related to Kasabach-Merritt Syndrome, but they do not specifically address treatment methods for a 3-year-old patient. The relevance is low due to the lack of direct applicability to the age group and condition specified in the query. Faithfulness is moderate as the abstracts mention KMS and its characteristics, but they do not provide concrete treatment options. Completeness is low as key aspects of treatment for a pediatric patient are not covered.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "현재 노드: gpt_query_rewriter\n",
      "GPT 재작성된 쿼리 (시도 4): 3세 Kasabach-Merritt Syndrome 환자의 일반적 치료법은?\n",
      "\n",
      "현재 노드: ensemble_search\n",
      "라우팅 평가: 점수 0.385 vs 임계값 0.600\n",
      "현재 시도 횟수: 4/4\n",
      "품질 충족 또는 최대 시도 도달: generate_answer\n",
      "\n",
      "현재 노드: llm_evaluation\n",
      "LLM 평가 결과 (시도 0):\n",
      "   관련성: 0.300\n",
      "   사실 일치도: 0.500\n",
      "   정보 완성도: 0.200\n",
      "   종합 점수: 0.385\n",
      "   권장 임계치: 0.600\n",
      "   점수가 낮아 쿼리를 다시 재작성합니다.\n",
      "   피드백: The retrieved abstracts provide some relevant information about Kasabach-Merritt Syndrome (KMS), particularly in terms of its characteristics and a specific case presentation. However, they do not adequately address general treatment methods for KMS, which is the focus of the user query. The relevance is moderate due to the mention of KMS, but the abstracts lack comprehensive treatment options. Faithfulness is somewhat supported by the factual information presented, but completeness is low as key aspects of treatment are missing.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "현재 노드: merge_outputs\n",
      "최종 답변:\n",
      "현재 문헌 검색으로는 질문하신 사항에 정확히 답변드리기 어렵습니다. 유사한 일반 소아 KMS 치료 지침을 참고하시거나, 질문을 구체화하여 다시 문의해 주세요.\n",
      "\n",
      "실행 완료!\n"
     ]
    }
   ],
   "source": [
    "# 열한 번째 셀: 실행 (Jupyter에서 await 직접 사용)\n",
    "# question = \"Kasabach-Merritt Syndrome의 일반적 치료법은?\"\n",
    "question = \"3세 Kasabach-Merritt Syndrome 환자의 일반적 치료법은?\"\n",
    "result = await run_chatbot(question)\n",
    "print(\"\\n실행 완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9c2a55",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
